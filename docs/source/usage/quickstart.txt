.. _quick_start:

Quick start
============

The principle pipeline which JMCtf is designed to streamline is
the following:

1. :ref:`combine_into_joint`
2. :ref:`sample_from_joint`
3. :ref:`build_model`  
4. :ref:`find_MLEs` for parameters (for many
   samples/trials, in parallel)
5. :ref:`build_test_stats` 

Getting more advanced, we can loop over the whole procedure to simultaneously test many alternate hypotheses and compute :ref:`trial_corrections`, as well as define new :ref:`analysis` classes to extend JMCtf with new component probability distributions.

A fast introduction to the package, then, is to see an example of this pipeline in action. So let's get to it!

.. _combine_into_joint:

Combine independent "analyses" into a joint distribution
--------------------------------------------------------------------

.. _scipy.stats: https://docs.scipy.org/doc/scipy/reference/stats.html

The basic components in JMCTF are "analyses". These are objects describing the joint PDF of a single "analysis" or "experiment", for example an experiment searching for a signal in a series of Poisson "bins" or signal regions, or simply a normal random variable representing an experimental measurement. But these "analysis" objects do more than just describe the PDF of the experiment; they are responsible for constructing the TensorFlow probability model for the experiment (basically the PDF), for providing good starting guesses for distribution parameters based on sample realizations, for scaling parameters such that their MLEs have approximately unit variance (to help out the TensorFlow minimisation routines), for providing Asimov samples, and for keeping track of the sample structure. :ref:`adding_new_analysis_classes` is somewhat involved so it is covered separately.

JMCTF is designed for jointly analysing many independent experiments simultaneously. So to begin, let us create two simple experiments. First, a simple normal random variable, characterised by a mean and standard deviation. This can be created from the :py:class:`.NormalAnalysis` class:

.. literalinclude:: /../examples/quickstart.py
   :start-after: make_norm
   :end-before: make_binned

where the arguments are simple a name for the experiment, followed by the mean and standard deviation of the normal random variable. 

Next, an experiment consisting of several independent Poisson random variables, whose means are characterised in terms of a "signal" parameter (either fixed, or to be fitted) and a second "background" parameter constrained by a normally distributed control measurement:

.. literalinclude:: /../examples/quickstart.py
   :start-after: make_binned
   :end-before: make_joint

where a full description of the required bin definition data is given in the :py:class`.BinnedAnalysis` class documentation. 

Not much can be done with these Analysis objects on their own; they act mainly as "blueprints" for experiments, to be used by other classes. The simplest such class is :py:class:`.JMCJoint`, which can do such things as jointly fit the parameters of many Analysis classes, and sample from the underlying distribution functions.

Creating a joint PDF object with :py:class:`.JMCJoint` is as simple as providing a list of component Analysis classes to the constructor:

.. literalinclude:: /../examples/quickstart.py
   :start-after: make_joint
   :end-before: get_structure

Fitting the joint PDF to some data requires that the data be provided in an appropriately structured dictionary, so that samples can be assigned to the correct Analysis. The correct structure to use can be revealed by the :py:class`.JMCJoint.get_sample_structure` function::

    >>>> joint.get_sample_structure()
    {'Test normal': {'x': 1, 'x_theta': 1}, 'Test binned': {'n': 2, 'x': 2}}

Here the outer dictionary keys give the "experiment" names, the inner dictionary keys give the names of the random variables in each "experiment", and the inner dictionary values give the dimension of those random variables. For example we defined two bins in our :py:class:`.BinnedAnalysis`, so the dimension of the bin count random variable `n`, and control variable `x`, is 2.

Knowing this information, we can construct a sample for the full joint PDF and compute its joint probability (density)::

    >>>> my_sample = {'Test normal': {'x': 4.3, 'x_theta': 0}, 'Test binned': {'n': [9,53], 'x': [0,0]}}
    >>>> joint.log_prob(my_sample)


Sample from the joint distribution
----------------------------------

Now that we have an object describing our joint PDF, we can sample
from it in a scipy.stats manner:

.. literalinclude:: /../../tests/docs_examples/quickstart.py
   :start-after: sample_pdf
   :end-before: check_pdf

We can also evaluate the joint PDF, and compare it to our samples to
check that they seem reasonable:

.. literalinclude:: /../../tests/docs_examples/quickstart.py
   :start-after: check_pdf
   :end-before: build_model

.. figure:: /../../tests/docs_examples/example_2D_joint.svg
   :scale: 50%
   :alt: pdf_VS_samples

   Contours of the PDF of the joint distribution, with samples overlayed.

.. _build_model:

Build relationships between model parameters and distribution parameters
------------------------------------------------------------------------

In JMCtools a `model` consists of two main components: a 
:py:class:`.JointModel`, and a list of functions which take some
abstract parameters and return the arguments needed to evaluate the
distribution functions managed by the :py:class:`.JointModel`. These
are combined via the :py:class:`.ParameterModel` class.

For example, if we leave the variances of our two normal distributions
fixed, and take the means as independent parameters, we can construct
a simple two parameter model as follows:

.. literalinclude:: /../../tests/docs_examples/quickstart.py
   :start-after: build_model
   :end-before: block_structure

For the purposes of efficiently finding maximum likelihood estimators
for these parameters, the :py:class:`.ParameterModel` class 
automatically infers the block structure of the model. That is, it
figures out which blocks of parameters are needed to evaluate which
distribution functions. In our example the two parameters 
independently fix the means of each normal distribution, so our
2D model can be broken down into two independent 1D models. We can
see that the :py:class:`.ParameterModel` object has noticed this
by inspecting its :code:`blocks` attribute:

.. literalinclude:: /../../tests/docs_examples/quickstart.py
   :start-after: block_structure
   :end-before: alt_model

which produces::

   >>> {(deps=['a'], submodels=[0]), (deps=['b'], submodels=[1])}

Here the output is telling us that one parameter block depends on the
parameter `a`, and fixes the arguments of the 0th component of the
:py:class:`.JointModel`, and a second parameter block depends on `b`
and fixes the 1th joint distribution component.

As a quick aside, it is useful to see what happens if the model
parameters correlate the arguments of the joint distribution
components:

.. literalinclude:: /../../tests/docs_examples/quickstart.py
   :start-after: alt_model
   :end-before: sim_data

which produces::

   >>> {(deps=['a', 'b'], submodels=[0, 1])}

So now, there is only one parameter block, that depends on both
parameters and fixes the arguments of both joint distribution
components. The important difference is that now this block will
require a 2D optimisation in order to locate the maximum 
likelihood estimators for `a` and `b`, whereas previously they
could be found by two independent 1D optimisations (which is
much faster).

.. _find_MLEs:

Find maximum likelihood estimators
----------------------------------

Now that we have a :py:class:`.ParameterModel`, we can use it
to find maximum likelihood estimators for all the parameters
that we have defined. This is made simple by the 
:py:func:`find_MLE_parallel` member function, which can
find MLEs for each simulated dataset, splitting the task
over multiple processes if desired. 

We simulated some data earlier using the :py:class:`.JointModel`
class, but we can also simulate it directly
from the :py:class:`.ParameterModel`:

.. literalinclude:: /../../tests/docs_examples/quickstart.py
   :start-after: sim_data
   :end-before: find_MLEs

Note that the shape of the simulated data is important. The
length of the last dimension is interpreted as the number of
draws from the joint distribution per `trial` or pseudoexperiment. 
In this way, one can easily find the MLEs given multiple independent
draws. But for simplicity we here do just one draw per experiment. 
(For more complicated scenarios where different components of the
joint distribution require different numbers of "draws", one must
manually construct the appropriate :py:class:`.JointModel` before
wrapping it in a :py:class:`.ParameterModel`.) 

Finding the MLEs requires setting some options for the chosen
optimisation method and then simply calling 
:py:func:`find_MLE_parallel`

.. literalinclude:: /../../tests/docs_examples/quickstart.py
   :start-after: find_MLEs
   :end-before: compute_stats

.. _build_test_stats:

Construct test statistics
-------------------------

.. _likelihood ratio: https://en.wikipedia.org/wiki/Likelihood-ratio_test 

The final step is to construct some test statistic of interest!
Here we will compute a `likelihood ratio`_ test statistic:

.. literalinclude:: /../../tests/docs_examples/quickstart.py
   :start-after: compute_stats

.. figure:: /../../tests/docs_examples/quickstart_LLR.svg
   :scale: 50%
   :alt: pdf_LLR

   Simulated distribution of likelihood ratio test statistic (red), and
   expected distribution according to asymptotic theory (black).
 
And that's it! Everything this package is good for is just an application
of the above pipeline to different problems.

The unadulterated code for the above examples can be viewed :ref:`here <quick start examples>`.

Limitations
-----------

.. _iminuit: https://iminuit.readthedocs.io/en/latest/ 
.. _curse of dimensionality: https://en.wikipedia.org/wiki/Curse_of_dimensionality 

Please note the following:

* The `grid` optimisation method is very fast for low-dimensional
  problems, and very slow for dimensions larger than about 2 due to
  the `curse of dimensionality`_. Note 
  also that results will be poor if the resolution of the grid is not
  sufficiently below the variance of the MLEs.

* The `minuit` optimisation method needs a little help from you in 
  order to get reliable results. The starting guess needs to put
  the minimiser in the correct global minima, and the step size needs
  to be small enough that Minuit doesn't jump out of this minima.
  For more tips on using Minuit see the `iminuit`_ documentation. 

* There are currently no global optimisers implemented. Thus, if
  finding MLEs for your parameters requires solving a difficult
  global optimisation problem then this package cannot help you,
  sorry!
