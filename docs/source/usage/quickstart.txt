.. _quick_start:

Not-so-quick start
============

The principal pipeline which JMCtf is designed to streamline is
the following:

1. :ref:`combine_into_joint`
2. :ref:`sample_from_joint`
3. :ref:`find_MLEs`
4. :ref:`build_test_stats` 

Getting more advanced, we can loop over the whole procedure to simultaneously test many alternate hypotheses and compute :ref:`trial_corrections`, as well as define new :ref:`analysis` classes to extend JMCtf with new component probability distributions.

A fast introduction to the package, then, is to see an example of this pipeline in action. So let's get to it!

.. _combine_into_joint:

Combine independent "analyses" into a joint distribution
--------------------------------------------------------

.. _scipy.stats: https://docs.scipy.org/doc/scipy/reference/stats.html

The basic components in JMCTF are "analyses". These are objects describing the joint PDF of a single "analysis" or "experiment", for example an experiment searching for a signal in a series of Poisson "bins" or signal regions, or simply a normal random variable representing an experimental measurement. But these "analysis" objects do more than just describe the PDF of the experiment; they are responsible for constructing the TensorFlow probability model for the experiment (basically the PDF), for providing good starting guesses for distribution parameters based on sample realizations, for scaling parameters such that their MLEs have approximately unit variance (to help out the TensorFlow minimisation routines), for providing Asimov samples, and for keeping track of the sample structure. :ref:`adding_new_analysis_classes` is somewhat involved so it is covered separately.

JMCTF is designed for jointly analysing many independent experiments simultaneously. So to begin, let us create two simple experiments. First, a simple normal random variable, characterised by a mean and standard deviation. This can be created from the :py:class:`.NormalAnalysis` class::

   >>> from JMCTF import NormalAnalysis, BinnedAnalysis, JointDistribution
   >>> norm = NormalAnalysis("Test normal", 5, 2)

where the arguments are simply a name for the experiment, followed by the mean and standard deviation of the normal random variable. 

Next, an experiment consisting of several independent Poisson random variables, whose means are characterised in terms of a "signal" parameter (either fixed, or to be fitted) and a second "background" parameter constrained by a normally distributed control measurement::

   >>> # (name, n, b, sigma_b)
   >>> bins = [("SR1", 10, 9, 2),
   ...         ("SR2", 50, 55, 4)]
   >>> binned = BinnedAnalysis("Test binned", bins)

where a full description of the required bin definition data is given in the :py:class`.BinnedAnalysis` class documentation. 

Not much can be done with these Analysis objects on their own; they act mainly as "blueprints" for experiments, to be used by other classes. The simplest such class is :py:class:`.JointDistribution`, which can do such things as jointly fit the parameters of many Analysis classes, and sample from the underlying distribution functions [#jointdist]_.

Creating a joint PDF object with :py:class:`.JointDistribution` is as simple as providing a list of component Analysis object to the constructor::

   >>> joint = JointDistribution([norm,binned])

Fitting the joint PDF to some data requires that the data be provided in an appropriately structured dictionary, so that samples can be assigned to the correct Analysis. The correct structure to use can be revealed by the :py:class`.JointDistribution.get_sample_structure` function::

    >>> joint.get_sample_structure()
    {'Test normal::x': 1, 'Test normal::x_theta': 1, 'Test binned::n': 2, 'Test binned::x': 2}

Here the dictionary keys are in the format `analysis_name::random_variable_name` and the values give the dimension of those random variables. For example we defined two bins in our :py:class:`.BinnedAnalysis`, so the dimension of the bin count random variable `Test binned::n`, and control variable `Test binned::x`, is 2.

Knowing this information, we can manually construct a sample for the full joint PDF and fit the joint PDF to it::

    >>> my_sample = {'Test normal::x': 4.3, \
    ...              'Test normal::x_theta': 0, \
    ...              'Test binned::n': [9,53], \
    ...              'Test binned::x': [0,0]}

    >>> q, joint_fitted, all_pars = joint.fit_all(my_sample)
    >>> print(all_pars)
    {'Test normal': 
     {'mu': <tf.Variable 'mu:0' shape=() dtype=float32, numpy=4.3>, 
      'theta': <tf.Variable 'theta:0' shape=() dtype=float32, numpy=0.0>, 
      'sigma_t': <tf.Tensor: id=57, shape=(), dtype=float32, numpy=0.0>}, 
    'Test binned': 
     {'s': <tf.Variable 's:0' shape=(2,) dtype=float32, numpy=array([ 0.        , -0.23735635], dtype=float32)>, 
      'theta': <tf.Variable 'theta:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>}}

The output is not so pretty because the parameters are TensorFlow objects. We can convert them to numpy for better viewing::

    >>> def to_numpy(d):
    ...    out = {}
    ...    for k,v in d.items():
    ...        if isinstance(v, dict): out[k] = to_numpy(v)
    ...        else: out[k] = v.numpy()
    ...    return out
    >>> print(to_numpy(all_pars))
    {'Test normal': 
      {'mu': 4.3, 'theta': 0.0, 'sigma_t': 0.0}, 
     'Test binned': 
      {'s': array([ 0.        , -0.23735635], dtype=float32), 
       'theta': array([0., 0.], dtype=float32)}}

The :py:class`.JointDistribution.fit_all` function fits all the free parameters in the full joint distribution to the supplied samples, and returns `q` (negative 2 times the log probability density for the samples under the fitted model(s)), a new `JointDistribution` object fitted to the samples (the original remains unchanged), and a dictionary of the fitted parameter values. 

Here we have only fit the PDF to one sample, however the power of JMCTF really lies in fitting the PDF to lots of samples quickly. Of course manually creating lots of samples is tedious and not very useful; a more standard workflow is to actually *sample* the samples from the PDF under some fixed "null hypothesis" parameter values. We cover this in the next section.

.. _sample_from_joint:

Sample from the joint distribution
----------------------------------

In the previous section, having fitted our joint PDF to a single sample, we obtained a second `JointDistribution` object as output, whose parameters are set to their maximum likelihood estimators given that sample. Its probability distribution is therefore now fully-specified, and we can sample from it::

   >>> samples = joint_fitted.sample(3)
   >>> print(to_numpy(samples))
   {'Test normal::x_theta': array([0., 0., 0.], dtype=float32), 
    'Test normal::x': array([10.776527 , 10.519511 ,  9.5911875], dtype=float32), 
    'Test binned::x': array([[[-3.0337536, -1.6310872]],
                             [[-4.0343146,  1.5532861]],
                             [[-2.800459 , -0.7948484]]], dtype=float32), 
    'Test binned::n': array([[[ 8., 40.]],
                             [[ 2., 50.]],
                             [[ 7., 60.]]], dtype=float32)}

As before, we can fit our distribution to these samples, and this time we will obtain maximum likelihood estimators for each sample independently::

    >>> q_3, joint_fitted_3, all_pars_3 = joint.fit_all(samples)
    >>> print(to_numpy(all_pars_3))>
    {'Test normal': 
     {'mu': array([10.776527 , 10.519511 ,  9.5911875], dtype=float32), 
      'theta': array([0., 0., 0.], dtype=float32), 
      'sigma_t': 0.0}, 
     'Test binned': 
     {'s': array([[[ 0.5640617 , -1.586598  ]],
                  [[-0.82253313, -0.7777322 ]],
                  [[ 0.22200736,  0.68772215]]], dtype=float32), 
      'theta': array([[[-1.5168768 , -0.4077718 ]],
                      [[-2.0171573 ,  0.38832152]],
                      [[-1.4002295 , -0.1987121 ]]], dtype=float32)}}

Note that `sigma_t` is not considered a free parameter in the `BinnedAnalysis` class, which is why it still only has one value (for more on this see the :py:class:`.BinnedAnalysis` class documentation).

Also note that we cannot fit to all the samples simultaneously, i.e. we don't use each sample as independent information all contributing simultaneously to knowledge of the underlying parameters. JMCTF is designed for performing Monte Carlo simulations of scientific experiments that run just once (such as a search for new particles at the Large Hadron Collider), so each sample is treated as pseudodata whose main purpose is to help us understand the distributions of test statistics. In this view each sample is an independent pseudo-experiment. If an experiment is in fact to be run multiple times in reality, then the PDF of the underlying Analysis class needs to reflect this by using random variables of the appropriate dimension; or for example by using two normal random variables rather than one if the experiment runs twice. 

But that is getting complicated, so back to the quick-start.

.. _find_MLEs:

Fit MLEs to samples under a hypothesis (or many hypotheses) 
-----------------------------------------------------------

In the above example, we generated samples from a distribution that was fit to some data that we manually invented. But that is a weird thing to do. More conventionally we want to generate samples under a pre-defined *null hypothesis*, and use them to understand the distribution of some test statistic under that hypothesis. To do this, it is the parameter values that we want to fix manually, not the samples. We can supply these to the constructor of a `JointDistribution` object, however we can also introspect a parameter-less `JointDistribution` to determine what parameters are required in the event that we want to fix them::

    >>> free, fixed, nuis = joint.get_parameter_structure()
    >>> print(free)
    {'Test normal': {'mu': 1}, 'Test binned': {'s': 2}}

As with the sample structure introspection, this dictionary tells us which free parameters we need to supply, and what their dimension should be. The other dictionaries, `fixed` and `nuis`, tell us the structure of `fixed` and `nuisance` parameters respectively. The analyses themselves know sensible null hypothesis values for these, so we don't have to supply them, though we do have to explictly ask for the default values by setting the 'nuisance' dictionary key to `None`. See **TODO** for more details.

So, let us define a null hypothesis, sample from it, and then find MLEs for all parameters for all those sample:

    >>> #TODO: Internally ensure float32 format to avoid this verbosity?
    >>> null = {'Test normal': {'mu': np.array([[0.]],dtype='float32'), 'nuisance': None}, 
    ...         'Test binned': {'s': np.array([[0., 0.]],dtype='float32'), 'nuisance': None}}
    >>> joint_null = joint.fix_parameters(null)
    >>> # or alternatively one can supply parameters to the constructor:
    >>> # joint_null = JointDistribution([norm,binned],null)
    >>> samples = joint_null.sample(3)
    >>> q_null, joint_fitted_null, all_pars_null = joint_null.fit_all(samples)
    >>> print(to_numpy(all_pars_null))
    {'Test normal': 
     {'mu': array([[[-2.4318216]],
                  [[ 0.7213395]],
                  [[-1.833349 ]]], dtype=float32), 
      'theta': array([[[0.]],
                      [[0.]],
                      [[0.]]], dtype=float32), 
      'sigma_t': 0.0}, 
     'Test binned': 
     {'s': array([[[ 0.68332815,  1.2073289 ]],
                  [[-0.5019169 ,  0.8478795 ]],
                  [[ 1.0781676 , -0.9483687 ]]], dtype=float32), 
      'theta': array([[[-0.7318874, -1.5432839]],
                      [[-0.0951565, -1.0360898]],
                      [[-1.4436939,  1.2477738]]], dtype=float32)}}
    >>> # Inspect shapes
    >>> print({k1: {k2: v2.shape for k2,v2 in v1.items()} 
    ...  for k1,v1 in to_numpy(all_pars_null).items()})
    {'Test normal': {'mu': (3, 1, 1), 
                     'theta': (3, 1, 1), 
                     'sigma_t': ()}, 
     'Test binned': {'s': (3, 1, 2), 
                     'theta': (3, 1, 2)}}

We now need to start discussing the input/output array shapes more carefully. You will notice that we have supplied the parameters as two-dimensional arrays, even though it seems like one dimension should be enough. This is because we can use the `JointDistribution` class to collect and fit samples from *many* hypothesis simultaneously. That is, we could supply many alternate input parameters at once (we will do this in section **TODO**). On the output side, the structure of the fitted MLEs also reflects this possibility. Take for example the shape of the MLE for the parameter `'s'` from the `Test binned` analysis. This is `(3, 1, 2)`: 3 is the sample dimension (we have three samples per hypothesis to fit), 1 is the hypothesis dimension (we only provided one input hypothesis), and 2 is the fundamental dimension of `'s'` (we have two bins in the analysis, each characterised by a single parameter).

For simplicity, JMCTF is restricted to this three-dimensional form. Only one dimension for hypotheses, one dimension for samples, and one dimension for parameters is permitted. If you want to create e.g. a matrix of parameters in a custom Analysis class, it will need to be flattened when returning it to higher JMCTF classes, and if you want a multidimensional array of samples then you will need to sample them in 1D and then reshape. Likewise, arrays of input hypotheses will need to be appropriately flattened.

**TODO** refer to more detailed shape discussion elsewhere?

.. _build_test_stats:

Build and analyse test statistics
---------------------------------

Now that we understand the basic machinery, we can start to do some statistics!



.. rubric:: Footnotes

(Note, the crappy formatting of these footnotes is fixed in later versions of the sphinx_rtd_theme, and should go away as soon as readthedocs adopt a new release of it)

.. [#jointdist] As you might guess from the name, the :py:class:`.JointDistribution` class inherits from :py:class:`tensorflow_probability.JointDistributionNamed`. So all the standard log probability and sampling methods etc. from tensorflow_probability are available.
